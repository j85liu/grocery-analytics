{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54243ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7be0267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset with 2440 rows\n",
      "Date range: 2024-01-01 00:00:00 to 2024-03-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Generate sample grocery store data\n",
    "\n",
    "def generate_grocery_data():\n",
    "    # Date range: 3 months of daily data\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    end_date = datetime(2024, 3, 1)\n",
    "    dates = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "    # Store and product info\n",
    "    stores = ['Store_001', 'Store_002', 'Store_003', 'Store_004', 'Store_005']\n",
    "    products = {\n",
    "        'Bananas': {'category': 'Fresh Produce', 'avg_price': 1.99, 'perishable': True},\n",
    "        'Apples': {'category': 'Fresh Produce', 'avg_price': 3.49, 'perishable': True},\n",
    "        'Milk': {'category': 'Dairy', 'avg_price': 4.29, 'perishable': True},\n",
    "        'Bread': {'category': 'Bakery', 'avg_price': 2.99, 'perishable': True},\n",
    "        'Canned Beans': {'category': 'Pantry', 'avg_price': 1.89, 'perishable': False},\n",
    "        'Rice': {'category': 'Pantry', 'avg_price': 5.99, 'perishable': False},\n",
    "        'Chicken Breast': {'category': 'Meat', 'avg_price': 8.99, 'perishable': True},\n",
    "        'Yogurt': {'category': 'Dairy', 'avg_price': 5.49, 'perishable': True}\n",
    "    }\n",
    "\n",
    "    # Generate data\n",
    "    data = []\n",
    "    for date in dates:\n",
    "        for store in stores:\n",
    "            for product, info in products.items():\n",
    "                # Add some seasonality and randomness\n",
    "                base_demand = np.random.poisson(50)\n",
    "\n",
    "                # Weekend boost\n",
    "                if date.weekday() >= 5:\n",
    "                    base_demand *= 1.3\n",
    "\n",
    "                # Perishables have more variation\n",
    "                if info['perishable']:\n",
    "                    base_demand *= np.random.uniform(0.7, 1.4)\n",
    "                \n",
    "                # Occasional stockouts (missing data)\n",
    "                if np.random.random() < 0.05:  # 5% chance of missing data\n",
    "                    quantity_sold = np.nan\n",
    "                    revenue = np.nan\n",
    "                else:\n",
    "                    quantity_sold = max(0, int(base_demand))\n",
    "                    price_variation = np.random.uniform(0.9, 1.1)  # Â±10% price variation\n",
    "                    unit_price = info['avg_price'] * price_variation\n",
    "                    revenue = quantity_sold * unit_price\n",
    "                \n",
    "                data.append({\n",
    "                    'date': date,\n",
    "                    'store_id': store,\n",
    "                    'product_name': product,\n",
    "                    'category': info['category'],\n",
    "                    'quantity_sold': quantity_sold,\n",
    "                    'unit_price': info['avg_price'] * np.random.uniform(0.9, 1.1),\n",
    "                    'revenue': revenue,\n",
    "                    'is_perishable': info['perishable']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_grocery_data()\n",
    "print(f\"Generated dataset with {len(df)} rows\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b609470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== DATASET OVERVIEW =======\n",
      "Shape:  {(2440, 8)}\n",
      "\n",
      "Column Types: \n",
      "date             datetime64[ns]\n",
      "store_id                 object\n",
      "product_name             object\n",
      "category                 object\n",
      "quantity_sold           float64\n",
      "unit_price              float64\n",
      "revenue                 float64\n",
      "is_perishable              bool\n",
      "dtype: object\n",
      "\n",
      "Memory usage: 0.53 MB\n"
     ]
    }
   ],
   "source": [
    "# Load and Explore data\n",
    "print(\"====== DATASET OVERVIEW =======\")\n",
    "print(f\"Shape: \" ,{df.shape})\n",
    "print(f\"\\nColumn Types: \")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d84c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF head:          date   store_id  product_name       category  quantity_sold  \\\n",
      "0 2024-01-01  Store_001       Bananas  Fresh Produce           56.0   \n",
      "1 2024-01-01  Store_001        Apples  Fresh Produce           37.0   \n",
      "2 2024-01-01  Store_001          Milk          Dairy           39.0   \n",
      "3 2024-01-01  Store_001         Bread         Bakery           47.0   \n",
      "4 2024-01-01  Store_001  Canned Beans         Pantry           43.0   \n",
      "\n",
      "   unit_price     revenue  is_perishable  \n",
      "0    1.853086  103.773343           True  \n",
      "1    3.289213  137.715664           True  \n",
      "2    4.110875  165.032744           True  \n",
      "3    3.160535  139.295303           True  \n",
      "4    1.930652   73.898005          False  \n",
      "\n",
      "DF tail:             date   store_id    product_name category  quantity_sold  \\\n",
      "2435 2024-03-01  Store_005           Bread   Bakery           69.0   \n",
      "2436 2024-03-01  Store_005    Canned Beans   Pantry           51.0   \n",
      "2437 2024-03-01  Store_005            Rice   Pantry           54.0   \n",
      "2438 2024-03-01  Store_005  Chicken Breast     Meat           58.0   \n",
      "2439 2024-03-01  Store_005          Yogurt    Dairy           45.0   \n",
      "\n",
      "      unit_price     revenue  is_perishable  \n",
      "2435    2.861167  220.436326           True  \n",
      "2436    1.717193  100.861876          False  \n",
      "2437    5.623691  318.278659          False  \n",
      "2438    8.131791  532.581376           True  \n",
      "2439    5.576985  232.240151           True  \n",
      "\n",
      "DF describe:                        date  quantity_sold   unit_price      revenue\n",
      "count                 2440    2321.000000  2440.000000  2321.000000\n",
      "mean   2024-01-31 00:00:00      55.435588     4.376687   242.386291\n",
      "min    2024-01-01 00:00:00      21.000000     1.701512    47.858083\n",
      "25%    2024-01-16 00:00:00      45.000000     2.565625   130.355695\n",
      "50%    2024-01-31 00:00:00      54.000000     3.849522   211.308370\n",
      "75%    2024-02-15 00:00:00      64.000000     5.718347   314.654059\n",
      "max    2024-03-01 00:00:00     115.000000     9.874611  1045.349959\n",
      "std                    NaN      14.319095     2.239134   142.119743\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration methods\n",
    "print(f\"DF head: \", df.head())\n",
    "print(f\"\\nDF tail: \", df.tail())\n",
    "print(f'\\nDF describe: ', df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c61fcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========DATASET INFO===========\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2440 entries, 0 to 2439\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   date           2440 non-null   datetime64[ns]\n",
      " 1   store_id       2440 non-null   object        \n",
      " 2   product_name   2440 non-null   object        \n",
      " 3   category       2440 non-null   object        \n",
      " 4   quantity_sold  2321 non-null   float64       \n",
      " 5   unit_price     2440 non-null   float64       \n",
      " 6   revenue        2321 non-null   float64       \n",
      " 7   is_perishable  2440 non-null   bool          \n",
      "dtypes: bool(1), datetime64[ns](1), float64(3), object(3)\n",
      "memory usage: 135.9+ KB\n",
      "None\n",
      "\n",
      "=== UNIQUE VALUES PER COLUMN ===\n",
      "store_id: 5 unique values\n",
      "  Values: ['Store_001' 'Store_002' 'Store_003' 'Store_004' 'Store_005']\n",
      "product_name: 8 unique values\n",
      "  Values: ['Bananas' 'Apples' 'Milk' 'Bread' 'Canned Beans' 'Rice' 'Chicken Breast'\n",
      " 'Yogurt']\n",
      "category: 5 unique values\n",
      "  Values: ['Fresh Produce' 'Dairy' 'Bakery' 'Pantry' 'Meat']\n",
      "is_perishable: 2 unique values\n",
      "  Values: [ True False]\n",
      "\n",
      "=== MISSING VALUES ===\n",
      "quantity_sold    119\n",
      "revenue          119\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"=========DATASET INFO===========\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n=== UNIQUE VALUES PER COLUMN ===\")\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object' or df[col].dtype == 'bool':\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "        if df[col].nunique() < 10:\n",
    "            print(f\"  Values: {df[col].unique()}\")\n",
    "\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d080f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SELECTING COLUMNS ===\n",
      "Bananas rows: 305\n",
      "Key columns shape: (2440, 5)\n",
      "\n",
      "=== FILTERING DATA ===\n",
      "High revenue transactions (>$200): 1232\n",
      "Fresh produce sales on weekends: 160\n",
      "High-selling dairy products: 214\n"
     ]
    }
   ],
   "source": [
    "# Column selection\n",
    "print(\"=== SELECTING COLUMNS ===\")\n",
    "# Single column (returns Series)\n",
    "bananas_sales = df['product_name'] == 'Bananas'\n",
    "print(f\"Bananas rows: {bananas_sales.sum()}\")\n",
    "\n",
    "# Multiple columns\n",
    "key_columns = df[['date', 'store_id', 'product_name', 'quantity_sold', 'revenue']]\n",
    "print(f\"Key columns shape: {key_columns.shape}\")\n",
    "\n",
    "# Boolean filtering\n",
    "print(\"\\n=== FILTERING DATA ===\")\n",
    "# High revenue transactions\n",
    "high_revenue = df[df['revenue'] > 200]\n",
    "print(f\"High revenue transactions (>$200): {len(high_revenue)}\")\n",
    "\n",
    "# Multiple conditions\n",
    "fresh_produce_weekends = df[\n",
    "    (df['category'] == 'Fresh Produce') & \n",
    "    (df['date'].dt.weekday >= 5)\n",
    "]\n",
    "print(f\"Fresh produce sales on weekends: {len(fresh_produce_weekends)}\")\n",
    "\n",
    "# Using query method (alternative syntax)\n",
    "dairy_high_sales = df.query('category == \"Dairy\" and quantity_sold > 60')\n",
    "print(f\"High-selling dairy products: {len(dairy_high_sales)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "910f9111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LABEL-BASED SELECTION (loc) ===\n",
      "        date  product_name  quantity_sold\n",
      "0 2024-01-01       Bananas           56.0\n",
      "1 2024-01-01        Apples           37.0\n",
      "2 2024-01-01          Milk           39.0\n",
      "3 2024-01-01         Bread           47.0\n",
      "4 2024-01-01  Canned Beans           43.0\n",
      "5 2024-01-01          Rice           42.0\n",
      "\n",
      "=== POSITION-BASED SELECTION (iloc) ===\n",
      "        date   store_id    product_name\n",
      "0 2024-01-01  Store_001         Bananas\n",
      "1 2024-01-01  Store_001          Apples\n",
      "2 2024-01-01  Store_001            Milk\n",
      "3 2024-01-01  Store_001           Bread\n",
      "4 2024-01-01  Store_001    Canned Beans\n",
      "5 2024-01-01  Store_001            Rice\n",
      "6 2024-01-01  Store_001  Chicken Breast\n",
      "7 2024-01-01  Store_001          Yogurt\n",
      "8 2024-01-01  Store_002         Bananas\n",
      "9 2024-01-01  Store_002          Apples\n",
      "\n",
      "Random sample:\n",
      "      store_id       category  quantity_sold\n",
      "0    Store_001  Fresh Produce           56.0\n",
      "100  Store_003         Pantry           50.0\n",
      "200  Store_001  Fresh Produce           53.0\n",
      "300  Store_003         Pantry           40.0\n"
     ]
    }
   ],
   "source": [
    "# loc and iloc examples\n",
    "print(\"=== LABEL-BASED SELECTION (loc) ===\")\n",
    "# Select specific rows and columns\n",
    "sample_data = df.loc[0:5, ['date', 'product_name', 'quantity_sold']]\n",
    "print(sample_data)\n",
    "\n",
    "print(\"\\n=== POSITION-BASED SELECTION (iloc) ===\")\n",
    "# Select by position\n",
    "first_10_rows_3_cols = df.iloc[:10, :3]\n",
    "print(first_10_rows_3_cols)\n",
    "\n",
    "# Select specific positions\n",
    "random_sample = df.iloc[[0, 100, 200, 300], [1, 3, 4]]\n",
    "print(\"\\nRandom sample:\")\n",
    "print(random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2315a32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASIC GROUPBY OPERATIONS ===\n",
      "Total revenue by store:\n",
      "store_id\n",
      "Store_001    112841.283108\n",
      "Store_002    110152.488576\n",
      "Store_003    112545.986465\n",
      "Store_004    112689.795020\n",
      "Store_005    114349.029300\n",
      "Name: revenue, dtype: float64\n",
      "\n",
      "Category-Store statistics:\n",
      "                   quantity_sold                  revenue                  \n",
      "                             sum       mean           sum        mean count\n",
      "category store_id                                                          \n",
      "Bakery   Store_001        3251.0  57.035088   9615.212817  168.687944    57\n",
      "         Store_002        3291.0  55.779661   9835.992280  166.711734    59\n",
      "         Store_003        3442.0  58.338983  10226.573082  173.331747    59\n",
      "         Store_004        3401.0  57.644068  10215.483583  173.143790    59\n",
      "         Store_005        3259.0  56.189655   9803.825642  169.031477    58\n",
      "Dairy    Store_001        6651.0  57.834783  32750.755384  284.789177   115\n",
      "         Store_002        6486.0  56.400000  31525.320971  274.133226   115\n",
      "         Store_003        6700.0  56.779661  32588.142249  276.170697   118\n",
      "         Store_004        6033.0  52.921053  30340.083665  266.141085   114\n",
      "         Store_005        6630.0  56.666667  32365.340734  276.626844   117\n",
      "\n",
      "Daily summary (first 10 days):\n",
      "            quantity_sold   revenue  store_id  product_name\n",
      "date                                                       \n",
      "2024-01-01         1950.0   8520.09         5             8\n",
      "2024-01-02         2044.0   8877.46         5             8\n",
      "2024-01-03         1980.0   8686.36         5             8\n",
      "2024-01-04         1859.0   7834.52         5             8\n",
      "2024-01-05         2078.0   8927.60         5             8\n",
      "2024-01-06         2477.0  10642.91         5             8\n",
      "2024-01-07         2618.0  11560.42         5             8\n",
      "2024-01-08         1874.0   8501.74         5             8\n",
      "2024-01-09         2061.0   8842.85         5             8\n",
      "2024-01-10         1783.0   7892.46         5             8\n"
     ]
    }
   ],
   "source": [
    "# GroupBy operations - core of data analysis\n",
    "print(\"=== BASIC GROUPBY OPERATIONS ===\")\n",
    "\n",
    "# Group by single column\n",
    "store_totals = df.groupby('store_id')['revenue'].sum()\n",
    "print(\"Total revenue by store:\")\n",
    "print(store_totals)\n",
    "\n",
    "# Group by multiple columns\n",
    "category_store_stats = df.groupby(['category', 'store_id']).agg({\n",
    "    'quantity_sold': ['sum', 'mean'],\n",
    "    'revenue': ['sum', 'mean', 'count']\n",
    "})\n",
    "print(\"\\nCategory-Store statistics:\")\n",
    "print(category_store_stats.head(10))\n",
    "\n",
    "# More complex aggregations\n",
    "daily_summary = df.groupby('date').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'revenue': 'sum',\n",
    "    'store_id': 'nunique',  # Number of unique stores\n",
    "    'product_name': 'nunique'  # Number of unique products\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nDaily summary (first 10 days):\")\n",
    "print(daily_summary.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a99a9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REVENUE PER UNIT BY PRODUCT ===\n",
      "product_name\n",
      "Chicken Breast    8.952420\n",
      "Rice              5.998613\n",
      "Yogurt            5.527598\n",
      "Milk              4.299103\n",
      "Apples            3.491078\n",
      "Bread             2.985886\n",
      "Bananas           1.988791\n",
      "Canned Beans      1.888744\n",
      "dtype: float64\n",
      "\n",
      "=== TRANSFORM OPERATIONS ===\n",
      "Sample with new columns:\n",
      "   product_name       category     revenue  store_rank_in_category  \\\n",
      "0       Bananas  Fresh Produce  103.773343                   449.0   \n",
      "1        Apples  Fresh Produce  137.715664                   304.0   \n",
      "2          Milk          Dairy  165.032744                   538.0   \n",
      "3         Bread         Bakery  139.295303                   212.0   \n",
      "4  Canned Beans         Pantry   73.898005                   564.0   \n",
      "\n",
      "   pct_of_category_revenue  \n",
      "0                 0.116292  \n",
      "1                 0.154328  \n",
      "2                 0.103424  \n",
      "3                 0.280289  \n",
      "4                 0.059682  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/b1v52j156gbfz83pmrnth3jc0000gn/T/ipykernel_63790/3422807802.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  product_efficiency = df.groupby('product_name').apply(revenue_per_unit)\n"
     ]
    }
   ],
   "source": [
    "# Custom aggregation functions\n",
    "def revenue_per_unit(group):\n",
    "    \"\"\"Calculate average revenue per unit sold\"\"\"\n",
    "    total_revenue = group['revenue'].sum()\n",
    "    total_quantity = group['quantity_sold'].sum()\n",
    "    return total_revenue / total_quantity if total_quantity > 0 else 0\n",
    "\n",
    "# Apply custom function\n",
    "product_efficiency = df.groupby('product_name').apply(revenue_per_unit)\n",
    "print(\"=== REVENUE PER UNIT BY PRODUCT ===\")\n",
    "print(product_efficiency.sort_values(ascending=False))\n",
    "\n",
    "# Transform operations (keeps original DataFrame size)\n",
    "print(\"\\n=== TRANSFORM OPERATIONS ===\")\n",
    "# Add store rank within each category\n",
    "df['store_rank_in_category'] = df.groupby('category')['revenue'].rank(ascending=False)\n",
    "\n",
    "# Add percentage of total category revenue\n",
    "df['pct_of_category_revenue'] = df.groupby('category')['revenue'].transform(\n",
    "    lambda x: x / x.sum() * 100\n",
    ")\n",
    "\n",
    "print(\"Sample with new columns:\")\n",
    "print(df[['product_name', 'category', 'revenue', 'store_rank_in_category', 'pct_of_category_revenue']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dfbf09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MISSING DATA ANALYSIS ===\n",
      "Missing values per column:\n",
      "quantity_sold              119\n",
      "revenue                    119\n",
      "store_rank_in_category     119\n",
      "pct_of_category_revenue    119\n",
      "dtype: int64\n",
      "\n",
      "Missing data percentage:\n",
      "quantity_sold              4.877049\n",
      "revenue                    4.877049\n",
      "store_rank_in_category     4.877049\n",
      "pct_of_category_revenue    4.877049\n",
      "dtype: float64\n",
      "\n",
      "=== HANDLING MISSING VALUES ===\n",
      "Original shape: (2440, 10), After dropping NaN: (2321, 10)\n",
      "After filling: 238 missing values remain\n"
     ]
    }
   ],
   "source": [
    "print(\"=== MISSING DATA ANALYSIS ===\")\n",
    "# Check missing values\n",
    "missing_summary = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# Percentage of missing values\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "print(f\"\\nMissing data percentage:\")\n",
    "print(missing_pct[missing_pct > 0])\n",
    "\n",
    "# Different strategies for handling missing data\n",
    "print(\"\\n=== HANDLING MISSING VALUES ===\")\n",
    "\n",
    "# Strategy 1: Drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "print(f\"Original shape: {df.shape}, After dropping NaN: {df_dropped.shape}\")\n",
    "\n",
    "# Strategy 2: Fill missing values\n",
    "df_filled = df.copy()\n",
    "\n",
    "# Fill missing quantities with median by product\n",
    "df_filled['quantity_sold'] = df_filled.groupby('product_name')['quantity_sold'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# Fill missing revenue based on quantity and unit price\n",
    "mask = df_filled['revenue'].isnull()\n",
    "df_filled.loc[mask, 'revenue'] = (\n",
    "    df_filled.loc[mask, 'quantity_sold'] * df_filled.loc[mask, 'unit_price']\n",
    ")\n",
    "\n",
    "print(f\"After filling: {df_filled.isnull().sum().sum()} missing values remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1556c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUPLICATE DATA ===\n",
      "Duplicate rows: 0\n",
      "Business logic duplicates (same date/store/product): 0\n",
      "Shape after removing duplicates: (2440, 10)\n",
      "\n",
      "=== DATA TYPE OPTIMIZATION ===\n",
      "Memory usage comparison:\n",
      "Original: 0.57 MB\n",
      "Optimized: 0.12 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DUPLICATE DATA ===\")\n",
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Check for duplicates based on specific columns\n",
    "business_duplicates = df.duplicated(subset=['date', 'store_id', 'product_name']).sum()\n",
    "print(f\"Business logic duplicates (same date/store/product): {business_duplicates}\")\n",
    "\n",
    "# Remove duplicates if they exist\n",
    "df_clean = df.drop_duplicates(subset=['date', 'store_id', 'product_name'])\n",
    "print(f\"Shape after removing duplicates: {df_clean.shape}\")\n",
    "\n",
    "print(\"\\n=== DATA TYPE OPTIMIZATION ===\")\n",
    "# Convert data types for better performance\n",
    "df_optimized = df_clean.copy()\n",
    "\n",
    "# Convert categories to categorical (saves memory)\n",
    "df_optimized['category'] = df_optimized['category'].astype('category')\n",
    "df_optimized['store_id'] = df_optimized['store_id'].astype('category')\n",
    "df_optimized['product_name'] = df_optimized['product_name'].astype('category')\n",
    "\n",
    "# Convert boolean to categorical if needed\n",
    "df_optimized['is_perishable'] = df_optimized['is_perishable'].astype('category')\n",
    "\n",
    "print(\"Memory usage comparison:\")\n",
    "print(f\"Original: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Optimized: {df_optimized.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfb72b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATE/TIME OPERATIONS ===\n",
      "Date components added:\n",
      "        date  year  month  day_of_week day_name  is_weekend\n",
      "0 2024-01-01  2024      1            0   Monday       False\n",
      "1 2024-01-01  2024      1            0   Monday       False\n",
      "2 2024-01-01  2024      1            0   Monday       False\n",
      "3 2024-01-01  2024      1            0   Monday       False\n",
      "4 2024-01-01  2024      1            0   Monday       False\n",
      "\n",
      "=== TIME-BASED ANALYSIS ===\n",
      "Monthly sales:\n",
      "year  month\n",
      "2024  1        286562.574006\n",
      "      2        266782.721592\n",
      "      3          9233.286871\n",
      "Name: revenue, dtype: float64\n",
      "\n",
      "Average daily revenue by day of week:\n",
      "day_name\n",
      "Friday       227.471113\n",
      "Monday       219.542079\n",
      "Saturday     291.220595\n",
      "Sunday       291.681926\n",
      "Thursday     228.582834\n",
      "Tuesday      225.760632\n",
      "Wednesday    224.435669\n",
      "Name: revenue, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DATE/TIME OPERATIONS ===\")\n",
    "# Ensure date column is datetime\n",
    "df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
    "\n",
    "# Extract date components\n",
    "df_clean['year'] = df_clean['date'].dt.year\n",
    "df_clean['month'] = df_clean['date'].dt.month\n",
    "df_clean['day_of_week'] = df_clean['date'].dt.dayofweek\n",
    "df_clean['day_name'] = df_clean['date'].dt.day_name()\n",
    "df_clean['is_weekend'] = df_clean['day_of_week'] >= 5\n",
    "\n",
    "print(\"Date components added:\")\n",
    "print(df_clean[['date', 'year', 'month', 'day_of_week', 'day_name', 'is_weekend']].head())\n",
    "\n",
    "# Time-based analysis\n",
    "print(\"\\n=== TIME-BASED ANALYSIS ===\")\n",
    "# Monthly sales trends\n",
    "monthly_sales = df_clean.groupby(['year', 'month'])['revenue'].sum()\n",
    "print(\"Monthly sales:\")\n",
    "print(monthly_sales)\n",
    "\n",
    "# Day of week analysis\n",
    "weekday_sales = df_clean.groupby('day_name')['revenue'].mean()\n",
    "print(f\"\\nAverage daily revenue by day of week:\")\n",
    "print(weekday_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88e35615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TIME SERIES OPERATIONS ===\n",
      "Weekly sales by product (first few entries):\n",
      "product_name  date      \n",
      "Apples        2024-01-07    6201.452094\n",
      "              2024-01-14    6935.808577\n",
      "              2024-01-21    6182.400752\n",
      "              2024-01-28    6189.143534\n",
      "              2024-02-04    6379.816632\n",
      "              2024-02-11    7238.881491\n",
      "              2024-02-18    6332.293400\n",
      "              2024-02-25    6601.991575\n",
      "              2024-03-03    4064.278902\n",
      "Bananas       2024-01-07    3894.140233\n",
      "Name: revenue, dtype: float64\n",
      "\n",
      "Monthly aggregated metrics:\n",
      "            quantity_sold    revenue  unit_price\n",
      "date                                            \n",
      "2024-01-31        65411.0  286562.57        4.38\n",
      "2024-02-29        61182.0  266782.72        4.37\n",
      "2024-03-31         2073.0    9233.29        4.35\n",
      "\n",
      "7-day rolling average revenue (last 10 days):\n",
      "date\n",
      "2024-02-21    9018.256225\n",
      "2024-02-22    9100.201843\n",
      "2024-02-23    9007.802961\n",
      "2024-02-24    8993.972082\n",
      "2024-02-25    9292.883513\n",
      "2024-02-26    9326.607930\n",
      "2024-02-27    9288.678825\n",
      "2024-02-28    9163.349191\n",
      "2024-02-29    9157.224947\n",
      "2024-03-01    9240.817494\n",
      "Name: revenue, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/b1v52j156gbfz83pmrnth3jc0000gn/T/ipykernel_63790/4141802592.py:11: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_metrics = df_ts.resample('M').agg({\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TIME SERIES OPERATIONS ===\")\n",
    "# Set date as index for time series operations\n",
    "df_ts = df_clean.set_index('date')\n",
    "\n",
    "# Resample to get weekly/monthly aggregations\n",
    "weekly_sales = df_ts.groupby('product_name')['revenue'].resample('W').sum()\n",
    "print(\"Weekly sales by product (first few entries):\")\n",
    "print(weekly_sales.head(10))\n",
    "\n",
    "# Monthly resampling with multiple metrics\n",
    "monthly_metrics = df_ts.resample('M').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'revenue': 'sum',\n",
    "    'unit_price': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(f\"\\nMonthly aggregated metrics:\")\n",
    "print(monthly_metrics)\n",
    "\n",
    "# Rolling averages (7-day moving average)\n",
    "daily_revenue = df_ts.groupby('date')['revenue'].sum()\n",
    "rolling_avg = daily_revenue.rolling(window=7).mean()\n",
    "\n",
    "print(f\"\\n7-day rolling average revenue (last 10 days):\")\n",
    "print(rolling_avg.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffd83e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MERGING AND JOINING DATA ===\n",
      "Store information:\n",
      "    store_id         city state  store_size_sqft manager\n",
      "0  Store_001     New York    NY             5000   Alice\n",
      "1  Store_002  Los Angeles    CA             7500     Bob\n",
      "2  Store_003      Chicago    IL             6000   Carol\n",
      "3  Store_004      Houston    TX             5500   David\n",
      "4  Store_005      Phoenix    AZ             6500     Eve\n",
      "\n",
      "Product cost information:\n",
      "     product_name  cost_per_unit   supplier\n",
      "0         Bananas            0.8  FreshCorp\n",
      "1          Apples            1.2  FreshCorp\n",
      "2            Milk            2.1    DairyCo\n",
      "3           Bread            1.5  BreadCorp\n",
      "4    Canned Beans            0.9    CanCorp\n",
      "5            Rice            2.5  GrainCorp\n",
      "6  Chicken Breast            4.5   MeatCorp\n",
      "7          Yogurt            2.2    DairyCo\n",
      "\n",
      "Enhanced dataset shape: (2440, 21)\n",
      "Sample of enhanced data:\n",
      "    store_id      city  product_name  cost_per_unit     revenue\n",
      "0  Store_001  New York       Bananas            0.8  103.773343\n",
      "1  Store_001  New York        Apples            1.2  137.715664\n",
      "2  Store_001  New York          Milk            2.1  165.032744\n",
      "3  Store_001  New York         Bread            1.5  139.295303\n",
      "4  Store_001  New York  Canned Beans            0.9   73.898005\n"
     ]
    }
   ],
   "source": [
    "print(\"=== MERGING AND JOINING DATA ===\")\n",
    "# Create additional datasets to demonstrate joins\n",
    "\n",
    "# Store information\n",
    "store_info = pd.DataFrame({\n",
    "    'store_id': ['Store_001', 'Store_002', 'Store_003', 'Store_004', 'Store_005'],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n",
    "    'state': ['NY', 'CA', 'IL', 'TX', 'AZ'],\n",
    "    'store_size_sqft': [5000, 7500, 6000, 5500, 6500],\n",
    "    'manager': ['Alice', 'Bob', 'Carol', 'David', 'Eve']\n",
    "})\n",
    "\n",
    "# Product cost information\n",
    "product_costs = pd.DataFrame({\n",
    "    'product_name': ['Bananas', 'Apples', 'Milk', 'Bread', 'Canned Beans', 'Rice', 'Chicken Breast', 'Yogurt'],\n",
    "    'cost_per_unit': [0.80, 1.20, 2.10, 1.50, 0.90, 2.50, 4.50, 2.20],\n",
    "    'supplier': ['FreshCorp', 'FreshCorp', 'DairyCo', 'BreadCorp', 'CanCorp', 'GrainCorp', 'MeatCorp', 'DairyCo']\n",
    "})\n",
    "\n",
    "print(\"Store information:\")\n",
    "print(store_info)\n",
    "print(\"\\nProduct cost information:\")\n",
    "print(product_costs)\n",
    "\n",
    "# Merge with main dataset\n",
    "df_enhanced = df_clean.merge(store_info, on='store_id', how='left')\n",
    "df_enhanced = df_enhanced.merge(product_costs, on='product_name', how='left')\n",
    "\n",
    "print(f\"\\nEnhanced dataset shape: {df_enhanced.shape}\")\n",
    "print(\"Sample of enhanced data:\")\n",
    "print(df_enhanced[['store_id', 'city', 'product_name', 'cost_per_unit', 'revenue']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9af4e802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIFFERENT JOIN TYPES ===\n",
      "Left join shape: (2440, 19)\n",
      "Inner join shape: (2440, 19)\n",
      "\n",
      "=== CONCATENATING DATAFRAMES ===\n",
      "Original shape: (2440, 15)\n",
      "Concatenated shape: (2440, 15)\n",
      "Concat with keys shape: (2440, 15)\n"
     ]
    }
   ],
   "source": [
    "# Different types of joins\n",
    "print(\"=== DIFFERENT JOIN TYPES ===\")\n",
    "\n",
    "# Left join (keep all rows from left DataFrame)\n",
    "left_join = df_clean.merge(store_info, on='store_id', how='left')\n",
    "print(f\"Left join shape: {left_join.shape}\")\n",
    "\n",
    "# Inner join (only matching rows)\n",
    "inner_join = df_clean.merge(store_info, on='store_id', how='inner')\n",
    "print(f\"Inner join shape: {inner_join.shape}\")\n",
    "\n",
    "# Multiple DataFrames with pd.concat\n",
    "print(\"\\n=== CONCATENATING DATAFRAMES ===\")\n",
    "\n",
    "# Split data by month to demonstrate concat\n",
    "jan_data = df_clean[df_clean['month'] == 1]\n",
    "feb_data = df_clean[df_clean['month'] == 2]\n",
    "mar_data = df_clean[df_clean['month'] == 3]\n",
    "\n",
    "# Concatenate back together\n",
    "concatenated = pd.concat([jan_data, feb_data, mar_data], ignore_index=True)\n",
    "print(f\"Original shape: {df_clean.shape}\")\n",
    "print(f\"Concatenated shape: {concatenated.shape}\")\n",
    "\n",
    "# Concatenate with keys to track source\n",
    "concat_with_keys = pd.concat([jan_data, feb_data, mar_data], \n",
    "                            keys=['January', 'February', 'March'])\n",
    "print(f\"Concat with keys shape: {concat_with_keys.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7c206ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIFFERENT JOIN TYPES ===\n",
      "Left join shape: (2440, 19)\n",
      "Inner join shape: (2440, 19)\n",
      "\n",
      "=== CONCATENATING DATAFRAMES ===\n",
      "Original shape: (2440, 15)\n",
      "Concatenated shape: (2440, 15)\n",
      "Concat with keys shape: (2440, 15)\n"
     ]
    }
   ],
   "source": [
    "# Different types of joins\n",
    "print(\"=== DIFFERENT JOIN TYPES ===\")\n",
    "\n",
    "# Left join (keep all rows from left DataFrame)\n",
    "left_join = df_clean.merge(store_info, on='store_id', how='left')\n",
    "print(f\"Left join shape: {left_join.shape}\")\n",
    "\n",
    "# Inner join (only matching rows)\n",
    "inner_join = df_clean.merge(store_info, on='store_id', how='inner')\n",
    "print(f\"Inner join shape: {inner_join.shape}\")\n",
    "\n",
    "# Multiple DataFrames with pd.concat\n",
    "print(\"\\n=== CONCATENATING DATAFRAMES ===\")\n",
    "\n",
    "# Split data by month to demonstrate concat\n",
    "jan_data = df_clean[df_clean['month'] == 1]\n",
    "feb_data = df_clean[df_clean['month'] == 2]\n",
    "mar_data = df_clean[df_clean['month'] == 3]\n",
    "\n",
    "# Concatenate back together\n",
    "concatenated = pd.concat([jan_data, feb_data, mar_data], ignore_index=True)\n",
    "print(f\"Original shape: {df_clean.shape}\")\n",
    "print(f\"Concatenated shape: {concatenated.shape}\")\n",
    "\n",
    "# Concatenate with keys to track source\n",
    "concat_with_keys = pd.concat([jan_data, feb_data, mar_data], \n",
    "                            keys=['January', 'February', 'March'])\n",
    "print(f\"Concat with keys shape: {concat_with_keys.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20ca7036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUSINESS ANALYSIS ===\n",
      "Profitability by product:\n",
      "                total_profit  profit_margin  quantity_sold    revenue  \\\n",
      "product_name                                                            \n",
      "Chicken Breast      69676.03          49.57        15667.0  140257.56   \n",
      "Rice                55161.63          58.07        15821.0   94904.05   \n",
      "Yogurt              53008.96          59.66        16157.0   89309.41   \n",
      "Apples              36636.17          65.36        16077.0   56126.07   \n",
      "Milk                35674.52          50.76        16343.0   70260.24   \n",
      "Bread               24569.51          49.44        16644.0   49697.09   \n",
      "Bananas             19611.54          59.49        16648.0   33109.40   \n",
      "Canned Beans        15155.01          52.21        15309.0   28914.78   \n",
      "\n",
      "                avg_profit_per_unit  \n",
      "product_name                         \n",
      "Chicken Breast                 4.45  \n",
      "Rice                           3.49  \n",
      "Yogurt                         3.28  \n",
      "Apples                         2.28  \n",
      "Milk                           2.18  \n",
      "Bread                          1.48  \n",
      "Bananas                        1.18  \n",
      "Canned Beans                   0.99  \n",
      "\n",
      "Store performance:\n",
      "                         revenue  total_profit  quantity_sold  profit_margin\n",
      "store_id  city                                                              \n",
      "Store_005 Phoenix      114349.03      62592.90        25865.0          54.74\n",
      "Store_004 Houston      112689.80      62532.46        25783.0          55.49\n",
      "Store_003 Chicago      112545.99      62006.33        25746.0          55.09\n",
      "Store_001 New York     112841.28      61844.84        25727.0          54.81\n",
      "Store_002 Los Angeles  110152.49      60516.84        25545.0          54.94\n"
     ]
    }
   ],
   "source": [
    "print(\"=== BUSINESS ANALYSIS ===\")\n",
    "# Calculate profit margins using the enhanced dataset\n",
    "df_enhanced['profit_per_unit'] = df_enhanced['unit_price'] - df_enhanced['cost_per_unit']\n",
    "df_enhanced['total_profit'] = df_enhanced['profit_per_unit'] * df_enhanced['quantity_sold']\n",
    "df_enhanced['profit_margin'] = (df_enhanced['profit_per_unit'] / df_enhanced['unit_price']) * 100\n",
    "\n",
    "# Profitability analysis\n",
    "profitability_analysis = df_enhanced.groupby('product_name').agg({\n",
    "    'total_profit': 'sum',\n",
    "    'profit_margin': 'mean',\n",
    "    'quantity_sold': 'sum',\n",
    "    'revenue': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "profitability_analysis['avg_profit_per_unit'] = (\n",
    "    profitability_analysis['total_profit'] / profitability_analysis['quantity_sold']\n",
    ").round(2)\n",
    "\n",
    "print(\"Profitability by product:\")\n",
    "print(profitability_analysis.sort_values('total_profit', ascending=False))\n",
    "\n",
    "# Store performance analysis\n",
    "store_performance = df_enhanced.groupby(['store_id', 'city']).agg({\n",
    "    'revenue': 'sum',\n",
    "    'total_profit': 'sum',\n",
    "    'quantity_sold': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "store_performance['profit_margin'] = (\n",
    "    store_performance['total_profit'] / store_performance['revenue'] * 100\n",
    ").round(2)\n",
    "\n",
    "print(f\"\\nStore performance:\")\n",
    "print(store_performance.sort_values('total_profit', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bdf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== KEY PERFORMANCE INDICATORS ===\")\n",
    "\n",
    "# Calculate KPIs that would be relevant for Guac AI\n",
    "kpis = {}\n",
    "\n",
    "# 1. Revenue per store per day\n",
    "kpis['avg_daily_revenue_per_store'] = df_enhanced.groupby(['store_id', 'date'])['revenue'].sum().mean()\n",
    "\n",
    "# 2. Best and worst performing products\n",
    "product_performance = df_enhanced.groupby('product_name')['revenue'].sum().sort_values(ascending=False)\n",
    "kpis['best_product'] = product_performance.index[0]\n",
    "kpis['worst_product'] = product_performance.index[-1]\n",
    "\n",
    "# 3. Perishable vs non-perishable revenue\n",
    "perishable_revenue = df_enhanced.groupby('is_perishable')['revenue'].sum()\n",
    "kpis['perishable_revenue_pct'] = (perishable_revenue[True] / perishable_revenue.sum()) * 100\n",
    "\n",
    "# 4. Weekend vs weekday sales\n",
    "weekend_comparison = df_enhanced.groupby('is_weekend')['revenue'].sum()\n",
    "kpis['weekend_revenue_boost'] = ((weekend_comparison[True] / weekend_comparison[False]) - 1) * 100\n",
    "\n",
    "# 5. Store size efficiency\n",
    "store_efficiency = df_enhanced.groupby('store_id').agg({\n",
    "    'revenue': 'sum',\n",
    "    'store_size_sqft': 'first'\n",
    "})\n",
    "store_efficiency['revenue_per_sqft'] = store_efficiency['revenue'] / store_efficiency['store_size_sqft']\n",
    "kpis['most_efficient_store'] = store_efficiency['revenue_per_sqft'].idxmax()\n",
    "\n",
    "print(\"Key Performance Indicators:\")\n",
    "for key, value in kpis.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nRevenue per square foot by store:\")\n",
    "print(store_efficiency[['revenue_per_sqft']].round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
